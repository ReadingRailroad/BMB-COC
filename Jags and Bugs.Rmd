---
title: "JAGS"
author: "Marty Simonson"
date: "December 3, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---


# JAGS work in ecostats:

```{r}
install.packages("rjags")
```

# linear regression of grizzly bear data

```{r}
# Example code: constant slope, all observation error
#  so fit a linear regression

library(rjags)

# the grizzly bear data
grizz <- read.csv('grizzly.csv')

# only consider 1980 and later
grizz80 <- grizz[grizz$year >= 1980,]

# assemble the model, data, and control parameters
grizz.m0 <- jags.model(
  'lreg.bug', 
  data=list(
    n=length(grizz80$N), 
    x = grizz80$year - 1980, # set intercept to be beginning of sample years, not year 0
    y=log(grizz80$N) ),
  n.chains=3, # sometimes 5
  n.adapt=100)

# sample the MCMC chains 1000 times as burn-in, ignore the samples. this gets thrown away
update(grizz.m0, 1000)

# sample another 1000 times, keep the variables of interest
grizz.fit0 <- coda.samples(grizz.m0, # phil prefers coda.samples
  c('sigma', 'b0', 'b1'), # define what variables we want back from model
  1000, # number of samples
  thin=1) # specify thinning if necessary

# jags.samples() does the same thing, but coda.samples returns
#   more conveniently formatted output

par(mar=c(3,3,1,0)+0.2, mgp=c(2,0.8,0))
#  shrink margins so that plots are readable

par(ask=T)
#  and pause after each page of plots

plot(grizz.fit0)
# calls plot.mcmc() in coda library
#   which draws trace and density plots
summary(grizz.fit0)


gelman.diag(grizz.fit0)
# Gelman-Rubin diagnostic, want to be 1 or almost 1... That shows convergence!

# if traceplots and GR statistic indicate we haven't yet converged, 
#   burn-in some more
update(grizz.m0, 10000)

# and draw more samples
grizz.fit0 <- coda.samples(grizz.m0, 
  c('sigma', 'b0', 'b1'),
  1000 )
plot(grizz.fit0)


# thin the chains (i.e. long chains but only return every 1 in 10 values. need 10k to get 1000 back)
grizz.fit0 <- coda.samples(grizz.m0, 
  c('sigma', 'b0', 'b1'),
  10000,
  thin=10 )
plot(grizz.fit0)
# output from coda.samples packaged as a mcmc.list object 
# functions in the coda package (loaded automatically)
#   do useful things with this sort of object

# we've already seen plot() and gelman.diag()

summary(grizz.fit0)
#  print summaries for each variable (perhaps vector valued)
#    named in the coda.samples
#  these pool over multiple chains

# if you want to manipulate the output yourself, some useful functions are:

varnames(grizz.fit0)
# return the variable names for each stored variable

temp <- grizz.fit0[,'b1']
#  extract b1 from all chains

temp <- grizz.fit0[,c('b0', 'b1')]
#  extract b0 and b1 from all chains

# if you want to concatenate values from all the chains, 
#  I've found it easiest to convert the mcmc.list to a matrix first

grizz.fit0m <- as.matrix(grizz.fit0)
#  concatenates all chains and returns a matrix, variables are columns

temp <- grizz.fit0m[,'b1']

par(ask=F)
# turn off pausing

plot(density(temp))
# smoothed density plot of a variable
xp <- seq(-2,2,0.02)
lines(xp, dnorm(xp, 0, 1/sqrt(0.001)), col=2)
# compare to prior  = N(0, 1/0.001)

# how do Bayesian results compare to frequentist results?
grizz.lm <- lm(log(N) ~ I(year-1980), data=grizz80)
summary(grizz.lm)
confint(grizz.lm)

```

linear regression in bugs language
 yi = b0 +bi*xi+ei
 
 where ei ~ normal(0,1/tau) and yi ~ Normal(mu_i, 1/tau)
 
 all observational error
 
 define relationships between variables
```{r}
model 
{

# data model
for (i in 1:n) {

  my[i] <- b0 + b1*x[i] # assigns mean y for each i
  y[i] ~ dnorm(my[i], tau) # assigns a probability distribution
  }
 
# priors  

# normals are specified as (mean, precision) [not variance]
b0 ~ dnorm(0, 0.001)
b1 ~ dnorm(0, 0.001)

# these are Gelman's sd ~ uniform priors for a variance
# tau is the precision = 1/variance = 1/sd^2 

tau <- pow(sigma, -2)
sigma ~ dunif(0,100) # this is between 0 and a large number (log scale)


}

```

# state space model
N0 is start point (assigned to latent variable 1)
for 2 to n, we calculate mean of Latent[t] = latent[t-1] + r
  could add process error to that equation, 

```{r}
# Example code: constant slope, no process error

library(rjags)

# the grizzly bear data
grizz <- read.csv('grizzly.csv')

# only consider 1980 and later
grizz80 <- grizz[grizz$year >= 1980,]

# assemble the model, data, and control parameters
grizz.m0b <- jags.model(
  'lregb.bug', 
  data=list(n=length(grizz80$N), y=log(grizz80$N) ),
  n.chains=3,
  n.adapt=100)

# sample the MCMC chains 1000 times as burn-in, ignore the samples
update(grizz.m0b, 1000) # usually require longer burn-ins

# sample another 1000 times, keep the specific variables
grizz.j0b <- coda.samples(grizz.m0b, 
  c('sigma', 'n0', 'r'),
  1000,
  thin=1)

# jags.samples() does the same thing, but coda.samples returns
#   more conveniently formatted output

par(mar=c(3,3,1,0)+0.2, mgp=c(2,0.8,0))
#  shrink margins so that plots are readable

par(ask=T)
#  and pause after each page of plots

plot(grizz.j0b)
# calls plot.mcmc() in coda library
#   which draws trace and density plots

gelman.diag(grizz.j0b)
# Gelman-Rubin diagnostic, want to be 1 or almost 1.

# traceplots and GR statistic indicate we haven't yet converged, 
#   so burnin some more
update(grizz.m0b, 10000)

# and draw more samples, keeping only every 10'th sample
grizz.j0b <- coda.samples(grizz.m0b, 
  c('sigma', 'n0', 'r'),
  10000,
  thin=10)

# output from coda.samples packaged as a mcmc.list object 
# functions in the coda package (loaded automatically)
#   do useful things with this sort of object

# we've already seen plot() and gelman.diag()

summary(grizz.j0b)
#  print summaries for each variable (perhaps vector valued)
#    named in the coda.samples
#  these pool over multiple chains

```

# state space model with error

```{r}
# Example code: constant slope

library(rjags)

# the grizzly bear data
grizz <- read.csv('grizzly.csv')

# only consider 1980 and later
grizz80 <- grizz[grizz$year >= 1980,]

# assemble the model, data, and control parameters
grizz.m1 <- jags.model(
  'exp.bug', 
  data=list(n=length(grizz80$N), y=log(grizz80$N) ),
  n.chains=3,
  n.adapt=100)

# sample the MCMC chains 1000 times as burn-in, ignore the samples
update(grizz.m1, 1000)

# sample another 1000 times, keep the specific variables
grizz.j1 <- coda.samples(grizz.m1, 
  c('sigma', 'b0', 'l0', 'my'),
  1000,
  thin=1)

# jags.samples() does the same thing, but coda.samples returns
#   more conveniently formatted output

par(mar=c(3,3,1,0)+0.2, mgp=c(2,0.8,0))
#  shrink margins so that plots are readable

par(ask=T)
#  and pause after each page of plots

plot(grizz.j1)
# calls plot.mcmc() in coda library
#   which draws trace and density plots

gelman.diag(grizz.j1)
# Gelman-Rubin diagnostic, want to be 1 or almost 1.

# traceplots and GR statistic indicate we haven't yet converged, 
#   so burnin some more
update(grizz.m1, 10000)

# and draw more samples
grizz.j1 <- coda.samples(grizz.m1, 
  c('sigma', 'b0', 'l0', 'my'),
  6000,
  thin=6)

# output from coda.samples packaged as a mcmc.list object 
# functions in the coda package (loaded automatically)
#   do useful things with this sort of object

# we've already seen plot() and gelman.diag()

summary(grizz.j1)
#  print summaries for each variable (perhaps vector valued)
#    named in the coda.samples
#  these pool over multiple chains

# if you want to manipulate the output yourself, some useful functions are:

varnames(grizz.j1)
# return the variable names for each stored variable

temp <- grizz.j1[,'sigma[1]']
#  extract sigma[1] from all chains

temp <- grizz.j1[,c('sigma[1]', 'sigma[2]')]
#  extract sigma[1] and sigma[2] from all chains

# if you want to concatenate values from all the chains, 
#  I've found it easiest to convert the mcmc.list to a matrix first

grizz.j1m <- as.matrix(grizz.j1)
#  concatenates all chains and returns a matrix, variables are columns

temp <- grizz.j1m[,'b0']

par(ask=F)
# turn off pausing


plot(density(temp))
# smoothed density plot of a variable

# some useful things for estimates over time
#   columns 3:20 are the my variable for each year

yearq <- apply(grizz.j1m[,3:20], 2, quantile, c(0.05,0.5,0.95))
# rows! are 0.05, median and 0.95 quantiles of posterior distribution
#   for each year

matplot(1980:1997, t(yearq), type='l',lty=c(3,1,3), col=1, 
  xlab='Year', ylab='Mean log abundance')

points(1980:1997, log(grizz80$N), pch=19, col=4)

# another plot of same information
plot(1980:1997, yearq[2,], type='l',
  xlab='Year', ylab='Mean log abundance')

segments(1980:1997, yearq[1,], 1980:1997, yearq[3,])

# with more room for the credible intervals
matplot(1980:1997, t(yearq), type='n', 
  xlab='Year', ylab='Mean log abundance')
lines(1980:1997, yearq[2,])
segments(1980:1997, yearq[1,], 1980:1997, yearq[3,])

# can add data to any of these plots:
points(1980:1997, log(grizz80$N), pch=19, col=4)


# -------------------------------

# last bit of code demonstrates why model selection in these models is so hard.
#   the process error tends to soak up any lack of fit

# define a new variable, late that is 1 in 1990 and 0 otherwise every other year
#  the state space model: N_t = N_{t-1} + r*late 
#  has a constant mean prior to 1990, a jump of r in 1990, 
#   followed by a constant mean after 1990

grizz80$late <- (grizz80$year==1990)+0
with(grizz80, plot(year, late))

# need to add x*r to the exp.bug code

jump.m1 <- jags.model(
  'expcov.bug', 
  data=list(n=length(grizz80$N), x = grizz80$late, y=log(grizz80$N) ),
  n.chains=3,
  n.adapt=100)

# sample the MCMC chains 10000 times as burn-in, ignore the samples
update(jump.m1, 10000)

# sample another 10000 times, keep the specific variables
jump.j1 <- coda.samples(jump.m1, 
  c('sigma', 'b0', 'l0', 'my'),
  10000,
  thin=10)

gelman.diag(jump.j1)
summary(jump.j1)

jump.j1m <- as.matrix(jump.j1)
#  concatenates all chains and returns a matrix, variables are columns

# some useful things for estimates over time
#   columns 3:20 are the my variable for each year

yearqj <- apply(jump.j1m[,3:20], 2, quantile, c(0.05,0.5,0.95))
# rows! are 0.05, median and 0.95 quantiles of posterior distribution
#   for each year

matplot(1980:1997, t(yearqj), type='l',lty=c(3,1,3), col=1, 
  xlab='Year', ylab='Mean log abundance')
matlines(1980:1997, t(yearq), lty=c(3,1,3), col=4)
legend('topleft', bty='n', lty=1, col=c(1,4), 
  legend=c('Constant with jump in 1990', 'Linear trend'))

par(mfrow=c(1,2))
plot(density(jump.j1m[,'sigma[1]']), main='', xlab='Process variance',
  xlim=c(0,0.4) )
lines(density(grizz.j1m[,'sigma[1]']), col=4)
legend('topright', bty='n', lty=1, col=c(1,4), 
  legend=c('Constant with jump in 1990', 'Linear trend'), cex=0.6)

plot(density(jump.j1m[,'sigma[2]']), main='', xlab='Observation variance',
  xlim=c(0,0.4) )
lines(density(grizz.j1m[,'sigma[2]']), col=4)




points(1980:1997, log(grizz80$N), pch=19, col=4)
```

